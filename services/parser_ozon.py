import requests
from bs4 import BeautifulSoup
import json
import re
import random
import time
from config import ZENROWS_API_KEY
from logs import get_logger

logger = get_logger("parser_ozon")


def make_request_with_retry(url, params, max_retries=3, timeout=60):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö.
    """
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries} –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞...")
            response = requests.get(url, params=params, timeout=timeout)
            response.raise_for_status()
            logger.info(f"‚úÖ –ó–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ —Å –ø–æ–ø—ã—Ç–∫–∏ {attempt}")
            return response

        except requests.exceptions.Timeout:
            logger.warning(f"‚è∞ –¢–∞–π–º–∞—É—Ç –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}/{max_retries}")
            if attempt < max_retries:
                wait_time = attempt * 2
                logger.info(f"‚è≥ –ñ–¥–µ–º {wait_time} —Å–µ–∫. –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                time.sleep(wait_time)

        except requests.exceptions.ConnectionError:
            logger.warning(f"üîå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}/{max_retries}")
            if attempt < max_retries:
                wait_time = attempt * 3
                logger.info(f"‚è≥ –ñ–¥–µ–º {wait_time} —Å–µ–∫. –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º...")
                time.sleep(wait_time)

        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                logger.warning(f"‚ö†Ô∏è Rate limit –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}/{max_retries}")
                if attempt < max_retries:
                    wait_time = attempt * 5
                    logger.info(f"‚è≥ –ñ–¥–µ–º {wait_time} —Å–µ–∫. –∏–∑-–∑–∞ rate limit...")
                    time.sleep(wait_time)
            else:
                logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {e.response.status_code} –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}/{max_retries}")
                if attempt < max_retries:
                    time.sleep(2)

        except Exception as e:
            logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}/{max_retries}: {e}")
            if attempt < max_retries:
                time.sleep(2)

    logger.error(f"‚ùå –í—Å–µ {max_retries} –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É—Å–ø–µ—à–Ω—ã")
    raise requests.RequestException(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")


def extract_all_characteristics(soup):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–∑ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –±–ª–æ–∫–æ–≤"""
    characteristics = {}

    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–∫–æ—Ä–æ—Ç–∫–∏–µ)
    short_block = soup.find("div", {"data-widget": "webShortCharacteristics"})
    if short_block:
        rows = short_block.find_all("div", class_="q6l_27")
        for row in rows:
            key_elem = row.find("span", class_="tsBodyM")
            value_elem = row.find("span", class_="tsBody400Small")
            if key_elem and value_elem:
                key = key_elem.get_text(strip=True)
                value = value_elem.get_text(strip=True)
                characteristics[key] = value

    # –ü–æ–ª–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–¥–ª–∏–Ω–Ω—ã–µ)
    full_block = soup.find("div", {"data-widget": "webCharacteristics"})
    if full_block:
        char_rows = full_block.find_all("div", class_=re.compile(r".*characteristic.*|.*row.*"))
        if not char_rows:
            char_rows = full_block.find_all("tr")
        if not char_rows:
            char_rows = full_block.find_all("div", recursive=True)

        for row in char_rows:
            key_elem = row.find("dt") or row.find("span", class_="tsBodyM") or row.find("div", class_=re.compile(
                r".*key.*|.*name.*"))
            value_elem = row.find("dd") or row.find("span", class_="tsBody400Small") or row.find("div",
                                                                                                 class_=re.compile(
                                                                                                     r".*value.*"))

            if key_elem and value_elem:
                key = key_elem.get_text(strip=True)
                value = value_elem.get_text(strip=True)
                if key and value and len(key) < 100:
                    characteristics[key] = value

    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤ JSON-LD
    scripts = soup.find_all("script", type="application/ld+json")
    for script in scripts:
        try:
            data = json.loads(script.string)
            if isinstance(data, dict) and "additionalProperty" in data:
                for prop in data["additionalProperty"]:
                    if "name" in prop and "value" in prop:
                        characteristics[prop["name"]] = prop["value"]
        except:
            continue

    return characteristics


def is_good_image_url(url: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL —Ö–æ—Ä–æ—à–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Ç–æ–≤–∞—Ä–∞"""
    if not url:
        return False

    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ URL –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å http –∏–ª–∏ //
    if not url.startswith(('http://', 'https://', '//')):
        return False

    url_lower = url.lower()

    # –ò—Å–∫–ª—é—á–∞–µ–º –ø–ª–æ—Ö–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏)
    bad_patterns = [
        '/video',  # –í–∏–¥–µ–æ –≤ –ø—É—Ç–∏
        'video-',  # –í–∏–¥–µ–æ –ø—Ä–µ—Ñ–∏–∫—Å
        'video/',  # –í–∏–¥–µ–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        '/cover.',  # –û–±–ª–æ–∂–∫–∏ –≤–∏–¥–µ–æ
        '/cover/',  # –û–±–ª–æ–∂–∫–∏ –≤–∏–¥–µ–æ –≤ –ø—É—Ç–∏
        'cover/wc',  # –û–±–ª–æ–∂–∫–∏ —Å —Ä–∞–∑–º–µ—Ä–æ–º
        'cover.jpg',  # –û–±–ª–æ–∂–∫–∏ jpg
        '/logo',  # –õ–æ–≥–æ—Ç–∏–ø—ã
        '/icon',  # –ò–∫–æ–Ω–∫–∏
        '/avatar',  # –ê–≤–∞—Ç–∞—Ä—ã
        'placeholder',  # –ó–∞–≥–ª—É—à–∫–∏
        'blank',  # –ü—É—Å—Ç—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        'loading',  # –ó–∞–≥—Ä—É–∑–æ—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        '.mp4',  # –í–∏–¥–µ–æ —Ñ–∞–π–ª—ã
        '.webm',  # –í–∏–¥–µ–æ —Ñ–∞–π–ª—ã
        '.avi',  # –í–∏–¥–µ–æ —Ñ–∞–π–ª—ã
        'wc50/',  # –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ
        'wc100/',  # –ú–∞–ª–µ–Ω—å–∫–∏–µ
        'wc200/',  # –ú–∞–ª–µ–Ω—å–∫–∏–µ
        'w50/',  # –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ
        'w100/',  # –ú–∞–ª–µ–Ω—å–∫–∏–µ
    ]

    for pattern in bad_patterns:
        if pattern in url_lower:
            return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    good_patterns = [
        '.jpg', '.jpeg', '.png', '.webp', '.gif',
        '/multimedia',  # Ozon multimedia CDN
        '/ir.ozone.ru',  # Ozon image CDN
        'ir-',  # Ozon image prefix
    ]

    return any(pattern in url_lower for pattern in good_patterns)


def improve_image_quality(url: str) -> str:
    """–£–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞–º–µ–Ω–æ–π —Ä–∞–∑–º–µ—Ä–æ–≤"""
    if not url:
        return url

    # –î–ª—è Ozon –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    if 'ozone.ru' in url or '/ir-' in url or 'ir.ozone.ru' in url:
        # –ó–∞–º–µ–Ω—è–µ–º –≤—Å–µ –º–∞–ª–µ–Ω—å–∫–∏–µ —Ä–∞–∑–º–µ—Ä—ã –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ
        sizes_to_replace = [
            ('wc50', 'wc1000'),
            ('wc100', 'wc1000'),
            ('wc200', 'wc1000'),
            ('wc250', 'wc1000'),
            ('wc300', 'wc1000'),
            ('wc400', 'wc1000'),
            ('wc500', 'wc1000'),
            ('wc600', 'wc1000'),
            ('wc700', 'wc1000'),
            ('wc800', 'wc1000'),
            ('wc900', 'wc1000'),
            ('w50', 'w1000'),
            ('w100', 'w1000'),
            ('w200', 'w1000'),
            ('w250', 'w1000'),
            ('w300', 'w1000'),
            ('w400', 'w1000'),
            ('w500', 'w1000'),
            ('w600', 'w1000'),
            ('w700', 'w1000'),
            ('w800', 'w1000'),
            ('w900', 'w1000'),
            ('ww50', 'wc1000'),  # –ï—â–µ –æ–¥–∏–Ω —Ñ–æ—Ä–º–∞—Ç
            ('ww100', 'wc1000'),
            ('ww200', 'wc1000'),
        ]

        improved_url = url
        for old_size, new_size in sizes_to_replace:
            if old_size in improved_url:
                improved_url = improved_url.replace(old_size, new_size)
                break

        return improved_url

    return url


def find_product_images(soup):
    """–ò—â–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–∞ –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö, –∏—Å–∫–ª—é—á–∞—è –≤–∏–¥–µ–æ-–æ–±–ª–æ–∂–∫–∏"""
    image_urls = []
    image_attributes = ['src', 'data-src', 'data-lazy-src', 'data-original', 'data-zoom-image', 'content']

    # –ü–†–ò–û–†–ò–¢–ï–¢ 1: Meta —Ç–µ–≥–∏ (–æ–±—ã—á–Ω–æ —Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
    meta_selectors = [
        ("meta[property='og:image']", "content"),
        ("meta[name='twitter:image']", "content"),
        ("meta[itemprop='image']", "content"),
    ]

    for selector, attr in meta_selectors:
        elements = soup.select(selector)
        for element in elements:
            url = element.get(attr)
            if url and is_good_image_url(url):
                improved = improve_image_quality(url)
                if improved not in image_urls:
                    image_urls.append(improved)
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ meta: {improved[:80]}...")

    # –ü–†–ò–û–†–ò–¢–ï–¢ 2: JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    scripts = soup.find_all("script", type="application/ld+json")
    for script in scripts:
        try:
            data = json.loads(script.string)
            if isinstance(data, dict):
                # –ò—â–µ–º image –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
                if "image" in data:
                    if isinstance(data["image"], str):
                        url = data["image"]
                        if is_good_image_url(url):
                            improved = improve_image_quality(url)
                            if improved not in image_urls:
                                image_urls.append(improved)
                                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ JSON-LD: {improved[:80]}...")
                    elif isinstance(data["image"], list):
                        for img in data["image"]:
                            if isinstance(img, str) and is_good_image_url(img):
                                improved = improve_image_quality(img)
                                if improved not in image_urls:
                                    image_urls.append(improved)
                            elif isinstance(img, dict) and "url" in img:
                                url = img["url"]
                                if is_good_image_url(url):
                                    improved = improve_image_quality(url)
                                    if improved not in image_urls:
                                        image_urls.append(improved)
        except:
            continue

    # –ü–†–ò–û–†–ò–¢–ï–¢ 3: –ì–∞–ª–µ—Ä–µ–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    gallery_selectors = [
        "div[data-widget='webGallery'] img",
        "[data-widget*='Gallery'] img",
        "[data-widget*='Photo'] img",
        "div.gallery img",
        "div[class*='gallery'] img",
        "div[class*='photo'] img",
        "div[class*='Image'] img",
        "div[class*='image'] img",
        "picture img",
        "img[src*='multimedia']",
        "img[data-src*='multimedia']",
    ]

    for selector in gallery_selectors:
        elements = soup.select(selector)
        for element in elements:
            for attr in image_attributes:
                src = element.get(attr)
                if src and is_good_image_url(src):
                    improved = improve_image_quality(src)
                    if improved not in image_urls:
                        image_urls.append(improved)

    # –ü–†–ò–û–†–ò–¢–ï–¢ 4: –ü–æ–∏—Å–∫ –≤ div —Å data-index (–∫–∞—Ä—É—Å–µ–ª—å)
    carousel_divs = soup.find_all("div", attrs={"data-index": True})
    for div in carousel_divs:
        imgs = div.find_all("img")
        for img in imgs:
            for attr in ['src', 'data-src', 'data-lazy-src']:
                src = img.get(attr)
                if src and is_good_image_url(src):
                    improved = improve_image_quality(src)
                    if improved not in image_urls:
                        image_urls.append(improved)

    # –ü–†–ò–û–†–ò–¢–ï–¢ 5: –õ—é–±—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å multimedia (–ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å)
    if len(image_urls) < 3:  # –ï—Å–ª–∏ –º–∞–ª–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—â–µ–º –µ—â–µ
        all_imgs = soup.find_all('img')
        for img in all_imgs:
            for attr in image_attributes:
                src = img.get(attr)
                if src and 'multimedia' in src and is_good_image_url(src):
                    improved = improve_image_quality(src)
                    if improved not in image_urls:
                        image_urls.append(improved)

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
    image_urls.sort(key=lambda x: (
        'wc1000' in x or 'w1000' in x,  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –±–æ–ª—å—à–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        'multimedia' in x,  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç multimedia
        not any(size in x for size in ['wc', 'w50', 'w100', 'w200'])  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –±–µ–∑ –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
    ), reverse=True)

    logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ {len(image_urls)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    return image_urls


def extract_brand(soup):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –±—Ä–µ–Ω–¥ —Ç–æ–≤–∞—Ä–∞"""
    brand_selectors = [
        "a[href*='/brand/']",
        "a.tsCompactControl500Medium[href*='/brand/']",
        "div.container h2",
        "a[data-widget='webBrand']",
        "span[data-widget='webBrand']",
        ".brand-name",
        "div[data-widget='webProductBrand']",
        "h2.brand",
        ".product-brand"
    ]

    for selector in brand_selectors:
        brand_elem = soup.select_one(selector)
        if brand_elem:
            brand = brand_elem.get_text(strip=True)
            if brand and len(brand) < 100:
                return brand

    return ""


def parse_ozon_with_zenrows_bs4(product_url: str, apikey: str = ZENROWS_API_KEY, max_retries: int = 3):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–∞ Ozon —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º"""
    logger.info(f"üîÑ –ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–∞ Ozon: {product_url}")

    endpoint = "https://api.zenrows.com/v1/"
    params = {
        "url": product_url,
        "apikey": apikey,
        "js_render": "true",
        "premium_proxy": "true",
        "wait": 12000,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ
        "wait_for": "img[src*='multimedia']",  # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ multimedia –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    }

    try:
        response = make_request_with_retry(endpoint, params, max_retries=max_retries, timeout=60)
        soup = BeautifulSoup(response.text, "html.parser")

        # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
        title_selectors = [
            "h1.zk4_27.tsHeadline550Medium",
            "h1[data-widget='webProductHeading']",
            "h1.product-title",
            ".product-name h1",
            "h1"
        ]

        title = ""
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                break

        # –ë—Ä–µ–Ω–¥
        brand = extract_brand(soup)

        # –¶–µ–Ω–∞
        price_selectors = [
            "span.y3k_27.ky2_27",
            ".price-current",
            "[data-widget='webPrice'] span",
            ".product-price span",
            "span[data-widget='webPrice']"
        ]

        price = ""
        for selector in price_selectors:
            price_elem = soup.select_one(selector)
            if price_elem:
                price = price_elem.get_text(strip=True)
                break

        # –û–ø–∏—Å–∞–Ω–∏–µ
        desc_selectors = [
            "div[data-widget='webDescription']",
            ".product-description",
            ".description-text",
            ".product-summary"
        ]

        description = ""
        for selector in desc_selectors:
            desc_elem = soup.select_one(selector)
            if desc_elem:
                description = desc_elem.get_text(strip=True)
                break

        # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        characteristics = extract_all_characteristics(soup)

        # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        if not description and characteristics:
            description = "\n".join([f"{k}: {v}" for k, v in list(characteristics.items())[:10]])

        # –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–û–ò–°–ö –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô
        image_urls = find_product_images(soup)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        filtered_images = []
        main_image = ""

        for url in image_urls:
            if is_valid_product_image(url):
                if not main_image:
                    main_image = url
                    logger.info(f"‚úÖ –í—ã–±—Ä–∞–Ω–æ –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {main_image[:100]}...")
                filtered_images.append(url)

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞
        if not main_image:
            logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ù–û: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è {product_url}")
            # –í—ã–≤–æ–¥–∏–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –ø–µ—Ä–≤—ã–µ URL –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if image_urls:
                logger.debug(f"   –ë—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã URL (–ø–µ—Ä–≤—ã–µ 3): {image_urls[:3]}")
            return None  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º None –µ—Å–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

        logger.info(
            f"‚úÖ –¢–æ–≤–∞—Ä Ozon —Å–ø–∞—Ä—à–µ–Ω: {len(characteristics)} —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫, {len(filtered_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
        )

        return {
            "title": title or "–ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
            "brand": brand,
            "price": price,
            "description": description,
            "characteristics": characteristics,
            "image_url": main_image,
            "all_images": filtered_images,
            "url": product_url,
            "source": "ozon"
        }

    except requests.RequestException as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ HTML –æ—Ç ZenRows: {e}")
        return None
    except Exception as e:
        logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–∞: {e}")
        return None


def parse_ozon_category_products(category_url: str, limit: int = 20, max_retries: int = 2) -> list:
    """
    –ü–∞—Ä—Å–∏—Ç —Ç–æ–≤–∞—Ä—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Ozon —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL —Ç–æ–≤–∞—Ä–æ–≤.
    """
    logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Ozon: {category_url}")

    endpoint = "https://api.zenrows.com/v1/"
    params = {
        "url": category_url,
        "apikey": ZENROWS_API_KEY,
        "js_render": "true",
        "premium_proxy": "true",
        "wait": 10000,
        "wait_for": "div[data-widget]",
    }

    try:
        response = make_request_with_retry(endpoint, params, max_retries=max_retries, timeout=90)
        html_content = response.text
        soup = BeautifulSoup(html_content, "html.parser")

        logger.info(f"üìÑ –†–∞–∑–º–µ—Ä HTML: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")
        logger.info(f"üè∑Ô∏è –ù–∞–π–¥–µ–Ω–æ —Ç–µ–≥–æ–≤ 'a': {len(soup.find_all('a'))}")

        product_urls = []

        logger.info("üîç –ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ —á–µ—Ä–µ–∑ regex...")

        patterns = [
            r'/product/[^"\s\'"]+',
            r'https://www\.ozon\.ru/product/[^"\s]+',
        ]

        for i, pattern in enumerate(patterns, 1):
            matches = re.findall(pattern, html_content)
            logger.info(f"   Pattern {i}: –Ω–∞–π–¥–µ–Ω–æ {len(matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")

            for match in matches:
                if match.startswith('/product/'):
                    full_url = f"https://www.ozon.ru{match}"
                else:
                    full_url = match

                full_url = full_url.split('?')[0].split('&')[0]

                if full_url not in product_urls and 'ozon.ru/product/' in full_url:
                    product_urls.append(full_url)

                if len(product_urls) >= limit:
                    break

            if len(product_urls) >= limit:
                break

        if product_urls:
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(product_urls)} —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
            return product_urls[:limit]
        else:
            logger.warning("‚ö†Ô∏è –¢–æ–≤–∞—Ä—ã –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback URLs")
            return get_fallback_ozon_urls()[:limit]

    except requests.RequestException as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Ozon –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫: {e}")
        logger.info("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback URLs")
        return get_fallback_ozon_urls()[:limit]
    except Exception as e:
        logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {e}")
        return get_fallback_ozon_urls()[:limit]


def get_fallback_ozon_urls() -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback URLs –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è - –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ —Å—Å—ã–ª–∫–∏"""
    return [
        "https://www.ozon.ru/product/kostyum-klassicheskiy-zanika-utsenennyy-tovar-2688390965/",
        "https://www.ozon.ru/product/rubashka-len-2241994907/?at=BrtzpE619CAG2JXjs7XVgANC4GjjzKIZBj7V0Sqx2635",
        "https://www.ozon.ru/product/palto-dreamwhite-1567177548/",
        "https://www.ozon.ru/product/pidzhak-pograni-2285590883/?at=x6tPnrMy4UP4781Zc8y8Ol6hYxvkgGFV9vQrXUAQo65q",
        "https://www.ozon.ru/product/kostyum-klassicheskiy-glodium-1645468997/"
    ]


def test_connection():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å ZenRows API"""
    try:
        logger.info("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å ZenRows...")
        test_url = "https://httpbin.org/ip"

        params = {
            "url": test_url,
            "apikey": ZENROWS_API_KEY,
        }

        response = make_request_with_retry("https://api.zenrows.com/v1/", params, max_retries=2, timeout=30)
        logger.info("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å ZenRows —Ä–∞–±–æ—Ç–∞–µ—Ç")
        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å ZenRows: {e}")
        return False


def is_valid_product_image(image_url: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –≤–∞–ª–∏–¥–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Ç–æ–≤–∞—Ä–∞"""
    if not image_url:
        return False

    # –°–ø–∏—Å–æ–∫ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π)
    bad_patterns = [
        'video',
        'cover',
        'logo',
        'icon',
        'avatar',
        'placeholder',
        'wc50',
        'wc100',
        'w50/',
        'w100/',
        '.mp4',
        '.webm',
    ]

    image_lower = image_url.lower()
    for pattern in bad_patterns:
        if pattern in image_lower:
            return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif']
    has_valid_extension = any(ext in image_lower for ext in valid_extensions)

    return has_valid_extension


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –¢–µ—Å—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
    test_url = "https://www.ozon.ru/product/tunika-larss-plyazhnaya-odezhda-1158405128/"

    logger.info(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {test_url}")
    product = parse_ozon_with_zenrows_bs4(test_url)

    if product:
        print("‚úÖ –¢–æ–≤–∞—Ä –Ω–∞–π–¥–µ–Ω:", product['title'])
        print("   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ:", product.get('image_url', '–ù–ï–¢ –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø'))
        if product.get('all_images'):
            print(f"   –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(product['all_images'])}")
    else:
        print("‚ùå –¢–æ–≤–∞—Ä –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –∏–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")